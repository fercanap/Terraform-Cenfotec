# 19. AWS Well-Architected Full Demo <!-- omit in toc -->

# [Estructura del proyecto](./assets/terraform-Page-5.drawio.png)  <!-- omit in toc -->
# 1. Root
## 1.1. Network
## 1.2. SecurityGroups
## 1.3. Ec2
## 1.4. LoadBalancer
## 1.5. AutoscalingGroups
## 1.6. DNS


- Cada módulo requiere de una carpeta con sus archivos.
- Usar tf validate, para comprobar las configuraciones.
- Usar -var para incluir las variables necesarias desde CLI.
- Usar terraform destroy -target RESOURCE_TYPE.NAME, para eliminar un recurso específico.
# 2. Configurar el proveedor AWS y la máquina virtual con las credenciales.
- usar la region: us-east-1
- usar las credenciales provistas para el laboratorio

# 3. Crear los recursos y variables GLOBALES del proyecto
## 3.1. Variables globales (variables.tf)
- lab_name: type: string, description: "Nombre del estudiante", condición: no menor de 3 caractéres
- key_name: type: string, description: "Nombre de la llave SSH para conectar con el Bastion host"
- cidr_block: type: string, descripcion: bloque de IP's para la VPC
- public_cidr_block_a: type: string, description = "Bloque de IP's para la subnet pública A"
- public_cidr_block_b: type: string, description = "Bloque de IP's para la subnet pública B"
- private_cidr_block: type: string, description = "Bloque de IP's para la subnet privada"
- public_zone_a: type: string, description = "Zona de acceso público A"
- public_zone_b: type: string, description = "Zona de acceso público B"
- private_zone: type: string, description = "Zona de acceso privado"
## 3.2. Valores por defecto (terraform.tfvars)
- cidr_block = "10.0.0.0/16"
- public_cidr_block_a  = "10.0.1.0/24"
- public_cidr_block_b  = "10.0.2.0/24"
- private_cidr_block = "10.0.3.0/24"
- public_zone_a = "us-east-1a"
- public_zone_b = "us-east-1b"
- private_zone = "us-east-1b"

## 3.3. Crear recurso data (./data.tf) con la imagen del sistema operativo Ubuntu 20.04
Este recurso data será utilizado por varios módulos.
```tf
data "aws_ami" "ubuntu" {
  most_recent = true

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  owners = ["099720109477"] # Canonical
}
```
## 3.4. TIP. Revisar el id del owner de la imagen
```
sudo apt install jq -y

aws ec2 describe-images --image-ids ami-0a695f0d95cefc163  --region us-east-2 | jq ".Images[0].OwnerId"
```

## 3.5. Crear llave SSH y exportarla a un archivo local (./key.tf)
>[key_pair](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/key_pair)
Esta llave se utilizará para acceder al Bastion host
aws_key_pair: crea una llave .pem en AWS y está llave se le asigna a las VM para su acceso por SSH
```tf
resource "tls_private_key" "privateKey" {
  algorithm = "RSA"
  rsa_bits  = 4096
}

resource "aws_key_pair" "ec2_key" {
  key_name   = var.key_name
  public_key = tls_private_key.privateKey.public_key_openssh
}

resource "local_file" "key_file" {
  content  = tls_private_key.privateKey.private_key_pem
  filename = "key.pem"
	file_permission   = "0400"
}
```

# 4. Módulo Network
## 4.1. Crear VPC
> [VPC lab](https://github.com/cachac/TerraformLabs/blob/main/18.Workspaces.md#82-servertf)
>

- usar el CIDR: var.cidr_block
- Agregarle el tag: Name = "VPC ${var.lab_name}"

## 4.2. Crear subnet pública
### 4.2.1. Subnet A
- usar el CIDR: var.public_cidr_block_a
- zona: var.public_zone_a
### 4.2.2. Subnet B
- usar el CIDR: var.public_cidr_block_b
- zona: var.public_zone_b
## 4.3. Crear subnet privada
- usar el CIDR: var.private_cidr_block
- zona: var.private_zone

## 4.4. Crear tablas de enrutamiento y gateways
Para dividir el tráfico de las subnets públicas y privadas se debe configurar:
### 4.4.1. Internet Gateway
Para la entrada de tráfico desde internet hacia la Subnet pública
```tf
resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.vpc.id
}
```
### 4.4.2. NAT Gateway
Para la salida de tráfico de la subnet privada por medio de la subnet pública
```tf
resource "aws_eip" "nat_elastic_ip" {
  vpc = true
}

resource "aws_nat_gateway" "ngw" {
  depends_on = [aws_internet_gateway.igw]

  allocation_id = aws_eip.nat_elastic_ip.id
  subnet_id     = aws_subnet.public_subnet_a.id
}
```

### 4.4.3. Public Route Tables
```tf
resource "aws_route_table" "rt_igw" {
  depends_on = [aws_internet_gateway.igw, aws_vpc.vpc]
  vpc_id     = aws_vpc.vpc.id
}

# route from 0.0.0.0/0 to igw
resource "aws_route" "publicRoute" {
  depends_on             = [aws_route_table.rt_igw]
  route_table_id         = aws_route_table.rt_igw.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = aws_internet_gateway.igw.id
}

# associations
resource "aws_route_table_association" "public_subnet_association_a" {
  subnet_id      = aws_subnet.public_subnet_a.id
  route_table_id = aws_route_table.rt_igw.id
}
resource "aws_route_table_association" "public_subnet_association_b" {
  subnet_id      = aws_subnet.public_subnet_b.id
  route_table_id = aws_route_table.rt_igw.id
}
```

### 4.4.4. Private Route Tables
```tf
# route table
resource "aws_route_table" "rt_nat" {
  depends_on = [aws_vpc.vpc, aws_nat_gateway.ngw]

  vpc_id = aws_vpc.vpc.id
}

# route from 0.0.0.0/0 to igw
resource "aws_route" "routeNat" {
  depends_on = [aws_route_table.rt_nat]

  route_table_id         = aws_route_table.rt_nat.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = aws_nat_gateway.ngw.id
}

# associations
resource "aws_route_table_association" "private_subnet_association_a" {
  subnet_id      = aws_subnet.private_subnet.id
  route_table_id = aws_route_table.rt_nat.id
}
```

## 4.5. Exportar los recursos:  vpc - subnets
Es necesario exportar los recursos para ser utilizado en los demás módulos del laboratorio.
```tf
output "vpc" {
  value = aws_vpc.vpc
}
output "public_subnet_a" {
  value = aws_subnet.public_subnet_a
}
output "public_subnet_b" {
  value = aws_subnet.public_subnet_b
}
output "private_subnet" {
  value = aws_subnet.private_subnet
}
```
> En adelante deberá interpretar cuales recursos deben exportarse de cada módulo.
# 5. CheckPoint
- llamar el módulo Network en ./main.tf
- pasar las variables necesarias al módulo
- aplicar la configuración del módulo

# 6. SecurityGroups

## 6.1. Crear una variable (variables.tf) que reciba el VPC del punto anterior
```tf
variable "vpc" {
	type = any
	description = "VPC del laboratorio"
}
```
## 6.2. Bastion Host
- Ver el ejemplo :
> [aws_security_group](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group)
- permite el tráfico SSH, puerto 22 tcp de entrada desde "0.0.0.0/0"
- permite todo el tráfico de salida
- usar el id de la vpc creada en el paso anterior

Ejemplo Ingress a puerto 22:
```tf
  from_port   = 22
  to_port     = 22
  protocol    = "tcp"
  cidr_blocks = ["0.0.0.0/0"]
```

## 6.3. Public
- permite el tráfico HTTP, puerto 80 de entrada desde "0.0.0.0/0"
- permite el tráfico HTTPs,puerto 443 de entrada desde "0.0.0.0/0"
- permite todo el tráfico de salida
- usar el id de la vpc creada en el paso anterior
## 6.4. Private
- permite el tráfico SSH, puerto 22 de entrada desde el security group del Bastion Host
- permite el tráfico HTTP, puerto 80 de entrada desde el security group público
- permite el tráfico HTTPs,puerto 443 de entrada desde el security group público
- permite todo el tráfico de salida
- usar el id de la vpc creada en el paso anterior

# 7. CheckPoint
- llamar el módulo SecurityGroups en ./main.tf
- pasar la variable vpc al módulo
- aplicar la configuración del módulo

# 8. EC2

## 8.1. Crear una máquina virtual de acceso por puerto 22 (SSH)
Esta en la VM que permite la entrada a nuestra infraestructura.
Por cuestiones de seguridad, es la única VM que permite el acceso libre desde Internet.

> [ec2](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance)

- instancia t3a.nano
- ami: utilizar el id del ami GLOBAL creada en el punto 3
```tf
ami         		= NOMBRE_VARIABLE_AMI_GLOBAL
```
- subnet pública
- security group bastion
```tf
vpc_security_group_ids      = [var.sgBastion.id]
```
- [asociar una ip pública](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance#associate_public_ip_address)
 ```tf
 associate_public_ip_address = true
 ```
- tag nombre: concatenar la palabra "bastion-" con la variable "lab_name"
```tf
	tags = {
		Name = "bastion-${var.lab_name}"
	}
```
- configuración de acceso por SSH
```tf
key_name                    = var.ssh_key.key_name
```
- Agregar las llave SSH creada al bastion host
```tf
...
  connection {
    type        = "ssh"
    host        = self.public_ip
    user        = "ubuntu" # usuario por defecto en AWS
    private_key = var.private_key_pem # llave privada: key.pem (tls_private_key.privateKey)
  }

  provisioner "file" {
    source      = "./key.pem"
    destination = "/home/ubuntu/.ssh/id_rsa" # directorio por defecto donde se ubican las llaves ssh
  }
```
## 8.2. Exportar ip pública de la VM
> [ec2_public_ip](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance#public_ip)


# 9. En ./output.tf crear una salida que muestre el comando ssh para conectar con la IP pública del Bastion Host
```tf
output "bastion_host" {
  value       = "ssh -i key.pem ubuntu@${module.ec2._BASTION_PUBLIC_IP_OUTPUT}"
  description = "bastion SSH"
}
```
# 10. Checkpoint
- Configurar el módulo de EC2 y pasar las variables requeridas.
- Ingresar al Bastion Host
 ```vim
 ssh -i KEY_NAME ubuntu@PUBLIC_IP
 ```
 > la llave ssh debe modificar sus permisos: chmod 400 KEY_NAME

# 11. LoadBalancer
## 11.1. Solicitar las credenciales AWS para crear ALB.
## 11.2. Target Group
> [target Group](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb_target_group)
Es el grupo de instancias a balancear
- balancea a instancias en puerto 80

```tf
resource "aws_lb_target_group" "tgWebserver" {
  name     = "tgWebserver"
  port     = 80
  protocol = "HTTP"
  vpc_id   = var.vpc.id
}
```

## 11.3. Application Load Balancer (internet facing)
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lb
- tipo "application"
- internal = false (expuesto a Internet)
- subnets: publicas A y B
- security group público
```tf
resource "aws_lb" "albWebserver" {
  name               = "albWebserver-${var.lab_name}"
  load_balancer_type = "application"
  internal           = false
  subnets            = [var.public_subnet_a.id, var.public_subnet_b.id]
  security_groups    = [var.sgPublic.id]
}
```
## 11.4. Listener
Direcciona el tráfico al Target Group (grupo de instancias) seleccionado.
- monitorea el puerto 80
- accion: "forward", envía el tráfico al Target Group (arn) seleccionado.
```tf
resource "aws_lb_listener" "listener_80_webserver" {
  load_balancer_arn = aws_lb.albWebserver.arn
  port              = "80"
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.tgWebserver.arn
  }
}
```
# 12. AutoScaling Groups
Autoscaling configura reglas para el escalado horizontal de instancias EC2.

## 12.1. Solicitar las credenciales para el laboratorio.
## 12.2. Launch config
Plantilla de VM para escalar nuevas instancias
> [launch config](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/launch_configuration)

```tf
resource "aws_launch_configuration" "launch_webserver" {
  image_id                    = var.ubuntu_ami.id
  name_prefix                 = var.lab_name
  instance_type               = "t3a.nano"
  spot_price                  = "0.016" # utiliza instancias SPOT para el laboratorio. No se considera para ambientes productivos
  associate_public_ip_address = false # false = no asigna ip pública, porque es una vm privada.
  security_groups             = [var.sgPrivate.id]
  key_name                    = var.ssh_key.key_name # permite el ingreso por SSH desde el bastion host

  user_data = templatefile("./config/app.sh", {}) # cuando crea la instancia ejecuta el script app.sh para la configuración de docker y la ejecución del contenedor de la aplicación.

  lifecycle {
    create_before_destroy = true
  }
}
```

## 12.3. Autoscaling Group
https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_group
https://www.tderflinger.com/en/ec2-spot-with-terraform

- Capacidad deseada = 1
- Capacidad mínima = 1
- Capacida máxima = 2
- Asigna el Target Group para balancear la carga hacia las instancias.
```tf
resource "aws_autoscaling_group" "asg_webserver" {
  name                      = "asg-${var.lab_name}"
  vpc_zone_identifier       = [var.private_subnet.id]
  launch_configuration      = aws_launch_configuration.launch_webserver.name
  desired_capacity          = 1
  min_size                  = 1
  max_size                  = 2
  health_check_grace_period = 300
  # health_check_type         = "ELB"

  # LOAD BALANCING
  target_group_arns = [var.tgWebserver_arn]

  termination_policies = ["NewestInstance"]
  suspended_processes  = ["Terminate"]

  instance_refresh {
    strategy = "Rolling"
    preferences {
      min_healthy_percentage = 50
    }
    triggers = ["tag"]
  }

  lifecycle {
    create_before_destroy = true
  }

  tag {
    key                 = "Name"
    value               = "ASG-${var.lab_name}"
    propagate_at_launch = true
  }
}

```

## 12.4. Policy
Políticas de escalado: CPU - Memoria
>[policies](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_policy)

- Escala las instancias EC2 cuando el CPU es mayor al 50%

```tf
resource "aws_autoscaling_policy" "target_tracking_webserver" {
  name                   = "CPU_Utilization"
  policy_type            = "TargetTrackingScaling"
  autoscaling_group_name = aws_autoscaling_group.asg_webserver.name
  # scaling_adjustment     = 1
  # adjustment_type        = "ChangeInCapacity"
  # cooldown               = 300
  target_tracking_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ASGAverageCPUUtilization"
    }

    target_value = 50.0
  }
}
```

## 12.5. Opcional: Obtener id's y IP privada de la vm en ejecución
```vim
LAB_NAME=<LAB_NAME>

# ID's
aws ec2 describe-instances --region=us-east-1 \
  --query "Reservations[*].Instances[*].{Name:Tags[?Key=='Name']|[0].Value,Status:State.Name,InstanceId:InstanceId}"  \
  --filters Name=instance-state-name,Values=running Name=tag-value,Values=ASG-$LAB_NAME

# IP
aws ec2 describe-instances --region=us-east-1 \
  --query 'Reservations[*].Instances[*].[PrivateIpAddress]' \
  --filters Name=instance-state-name,Values=running Name=tag-value,Values=ASG-$LAB_NAME
```

## 12.6. Opcional: Detener VM's
Solo en caso de problemas al eliminar el Autoscaling Group
```vim
aws ec2  stop-instances --region us-east-1 --instance-ids  VM_ID
```

# 13. Checkpoint
- Ingresar al bastion host por ssh
- Ingresar por ssh a la VM de la aplicación creada por AutoScalingGroup
- Validar el contenedor corriendo la aplicación web:
- - sudo docker ps
- - sudo docker logs dockerlabs
- - curl localhost
- Opcional: escalar la vm utilizando el 80% del CPU
```vim
sudo apt install stress-ng -y
stress-ng --matrix 1 -t 1m
```

# 14. DNS
## 14.1. Configurar el registro DNS en la zona existente
- Crear la variable global domain = "aws-terralabs.tk"

## 14.2. La zona ya existe en la cuenta AWS. No es necesario crearla.
Se creó previamente utilizando el siguiente bloque con el lifecycle prevent_destroy
```tf
resource "aws_route53_zone" "main" {
  name = var.domain
  # lifecycle {
  #   prevent_destroy = true
  # }
}
```

## 14.3. Crear los registros DNS
- El nombre del registro se compone del nombre del laboratorio + dominio

- Registro tipo CNAME apuntando al dns_name del balanceador creado anteriormente
```tf
resource "aws_route53_record" "main" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "${var.lab_name}.${var.domain}"
  type    = "CNAME"
  ttl     = 300
  records = [var.albWebserver.dns_name]
}
```

# 15. Checkpoint
- Validar en el browser el url LAB_NAME.aws-terralabs.tk

# 16. Eliminar todos los recursos de AWS
